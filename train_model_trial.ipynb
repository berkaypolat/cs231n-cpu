{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as tfunc\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from PIL import Image\n",
    "import torch.nn.functional as func\n",
    "from util_datasets import GaussianNoise, UniformNoise\n",
    "import csv\n",
    "\n",
    "from sklearn.metrics.ranking import roc_auc_score\n",
    "import sklearn.metrics as metrics\n",
    "import random\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(123)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "\n",
    "class_names = ['No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity', \n",
    "               'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis', 'Pneumothorax', \n",
    "               'Pleural Effusion', 'Pleural Other', 'Fracture', 'Support Devices']\n",
    "\n",
    "imgtransResize = (320, 320)\n",
    "imgtransCrop = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer import CheXpertTrainer \n",
    "from chexpertClass import CheXpertData\n",
    "from denseNet121 import DenseNet121\n",
    "from utils import load_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRANSFORM DATA SEQUENCE\n",
    "normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "transformList = []\n",
    "#transformList.append(transforms.Resize(imgtransCrop))\n",
    "transformList.append(transforms.RandomResizedCrop(imgtransCrop))\n",
    "transformList.append(transforms.RandomHorizontalFlip())\n",
    "transformList.append(transforms.ToTensor())\n",
    "transformList.append(normalize)      \n",
    "transformSequence=transforms.Compose(transformList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CheXpert dataset loadig\n",
    "chex_datasetValid = CheXpertData('datasets/chexpert-small/CheXpert-v1.0-small/valid.csv' ,transformSequence, preload = True, policy=\"ones\")\n",
    "chex_datasetTrain = CheXpertData('datasets/chexpert-small/CheXpert-v1.0-small/train.csv' ,transformSequence, policy=\"ones\")\n",
    "datasetTest, datasetTrain = random_split(chex_datasetTrain, [500, len(chex_datasetTrain) - 500])\n",
    "#for model train testing purposes\n",
    "chex_valid = torch.utils.data.ConcatDataset([chex_datasetValid, datasetTest])\n",
    "dataLoaderChex = DataLoader(dataset=chex_valid, batch_size=10, shuffle=True,  num_workers=1, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NIH dataset loading\n",
    "nih_dataset = datasets.ImageFolder(root='datasets/nih-small/small', transform = transformSequence)\n",
    "nih_test, nih_train = random_split(nih_dataset, [734, len(nih_dataset) - 734])\n",
    "dataLoaderNIH = DataLoader(dataset=nih_test, batch_size=64, shuffle=False,  num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DenseNet121(len(class_names)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.DataParallel(model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded from cheXpert_github/model_ones_3epoch_densenet.tar\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = 'cheXpert_github/model_ones_3epoch_densenet.tar'\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-5)\n",
    "load_checkpoint(checkpoint_path, model, optimizer, use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_checkpoint\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "class CheXpertTrainer():\n",
    "\n",
    "    def train (self, model, dataLoaderTrain, nnClassCount, trMaxEpoch, checkpoint, use_cuda):\n",
    "        \n",
    "        #SETTINGS: OPTIMIZER & SCHEDULER\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-5)\n",
    "                \n",
    "        #SETTINGS: LOSS\n",
    "        loss = torch.nn.BCELoss(size_average = True)\n",
    "        \n",
    "        #LOAD CHECKPOINT \n",
    "        if checkpoint != None:\n",
    "            load_checkpoint(checkpoint, model, optimizer, use_cuda)\n",
    "\n",
    "        budget = 0.3\n",
    "        \n",
    "        #TRAIN THE NETWORK\n",
    "        \n",
    "        for epochID in range(0, trMaxEpoch):\n",
    "            \n",
    "            \n",
    "            batchs, losst = CheXpertTrainer.epochTrain(model, dataLoaderTrain, optimizer, \n",
    "                                                    trMaxEpoch, nnClassCount, loss, budget)\n",
    "         \n",
    "        return batchs, losst #, losse        \n",
    "    #-------------------------------------------------------------------------------- \n",
    "       \n",
    "    def epochTrain(model, dataLoader, optimizer, epochMax, classCount, loss, budget):\n",
    "        \n",
    "        batch = []\n",
    "        losstrain = []\n",
    "        \n",
    "        lmbda = 0.1    #start with reasonable value\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        for batchID, (varInput, target) in enumerate(dataLoader):\n",
    "            \n",
    "            batch.append(batchID)\n",
    "            varTarget = torch.stack(target).float().transpose(0,1).to(device)\n",
    "            print(varTarget.shape)\n",
    "            #varTarget = target.cuda(non_blocking = True)\n",
    "            \n",
    "            #varTarget = target.cuda()         \n",
    "\n",
    "            bs, c, h, w = varInput.size()\n",
    "            varInput = varInput.view(-1, c, h, w)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            varOutput, confidence = model(varInput)\n",
    "            confidence = torch.sigmoid(confidence)\n",
    "            \n",
    "            \n",
    "            # prevent any numerical instability\n",
    "            eps = 1e-12\n",
    "            varOutput = torch.clamp(varOutput, 0. + eps, 1. - eps)\n",
    "            confidence = torch.clamp(confidence, 0. + eps, 1. - eps)\n",
    "            \n",
    "            # Randomly set half of the confidences to 1 (i.e. no hints)\n",
    "            b = Variable(torch.bernoulli(torch.Tensor(confidence.size()).uniform_(0, 1))).to(device)\n",
    "            conf = confidence * b + (1 - b)\n",
    "            pred_new = varOutput * conf + varTarget * (1 - conf)\n",
    "            \n",
    "            first_loss = loss(pred_new, varTarget)\n",
    "            second_loss = torch.mean(torch.mean(-torch.log(confidence),1))\n",
    "            \n",
    "            loss_value = first_loss + lmbda * second_loss\n",
    "            \n",
    "            if budget > second_loss.item():\n",
    "                lmbda = lmbda / 1.01\n",
    "            elif budget <= second_loss.item():\n",
    "                lmbda = lmbda / 0.99\n",
    "            \n",
    "            \n",
    "            \n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            l = loss_value.item()\n",
    "            losstrain.append(l)\n",
    "            print(l)\n",
    "            \n",
    "        return batch, losstrain #, losseval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded from cheXpert_github/model_ones_3epoch_densenet.tar\n",
      "torch.Size([10, 14])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "can't alloc",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-f60634437bc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCheXpertTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbatchs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlost_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataLoaderChex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-c726ed9297a7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model, dataLoaderTrain, nnClassCount, trMaxEpoch, checkpoint, use_cuda)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             batchs, losst = CheXpertTrainer.epochTrain(model, dataLoaderTrain, optimizer, \n\u001b[0;32m---> 27\u001b[0;31m                                                     trMaxEpoch, nnClassCount, loss, budget)\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatchs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosst\u001b[0m \u001b[0;31m#, losse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-c726ed9297a7>\u001b[0m in \u001b[0;36mepochTrain\u001b[0;34m(model, dataLoader, optimizer, epochMax, classCount, loss, budget)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mloss_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: can't alloc"
     ]
    }
   ],
   "source": [
    "trainer = CheXpertTrainer()\n",
    "epochs = 1\n",
    "batchs, lost_train = trainer.train(model,dataLoaderChex, len(class_names), epochs, checkpoint_path, use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
